<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>teleported.in</title>
    <link>http://teleported.in/</link>
    <description>Recent content on teleported.in</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Thu, 02 Nov 2017 23:27:27 -0400</lastBuildDate>
    
	<atom:link href="http://teleported.in/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Decoding the ResNet architecture</title>
      <link>http://teleported.in/posts/decoding-resnet-architecture/</link>
      <pubDate>Thu, 02 Nov 2017 23:27:27 -0400</pubDate>
      
      <guid>http://teleported.in/posts/decoding-resnet-architecture/</guid>
      <description>Introduction Fast.ai&amp;rsquo;s 2017 batch kicked off on 30th Oct and Jeremy Howard introduced us participants to the ResNet model in the first lecture itself. I had used this model earlier in the passing but got curious to dig into its architecture this time. (In fact in one of my earlier client projects I had used Faster RCNN, which uses a ResNet variant under the hood.)
ResNet was unleashed in 2015 by Kaiming He.</description>
    </item>
    
    <item>
      <title>The Batch Normalization Technique</title>
      <link>http://teleported.in/posts/batch-normalization-explained/</link>
      <pubDate>Mon, 02 Oct 2017 23:27:27 -0400</pubDate>
      
      <guid>http://teleported.in/posts/batch-normalization-explained/</guid>
      <description>The Batch Normalization technique was proposed by researchers Sergey Ioffe, Christian Szegedy in a 2015 paper called Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift. Since then, it has become an integral part of any deep network architecture.
It has been one of the most exiting innovations in recent times, having impacted
The paper claims the following:
 It makes higher learning rates possible It makes proper weights initialization less critical Acts as a regularizer, in some cases eliminating the need for dropouts  We will be hashing each of this claims in this post.</description>
    </item>
    
    <item>
      <title>Analysing AlphaGo</title>
      <link>http://teleported.in/posts/analysing-alphago/</link>
      <pubDate>Fri, 12 May 2017 23:27:27 -0400</pubDate>
      
      <guid>http://teleported.in/posts/analysing-alphago/</guid>
      <description>(I wrote this piece as part of an assignment for Udacity&amp;rsquo;s AI Nanodegree program. The assignment was to summarize the AlphaGo paper in a page)
Introduction Go is a two player, turn taking, deterministic game of perfect information. Two main factors make Go very complex to solve:
 Go has an average branching factor ‘b’ of ~250 options per node (chess ~35) Go has an average depth ‘d’ of ~150 moves (chess ~80)  These factos make the state space of Go (bd) enormous to search end to end using traditional techniques.</description>
    </item>
    
    <item>
      <title>Visualising AI Search Algorithms</title>
      <link>http://teleported.in/posts/ai-search-algorithms/</link>
      <pubDate>Fri, 05 May 2017 23:27:27 -0400</pubDate>
      
      <guid>http://teleported.in/posts/ai-search-algorithms/</guid>
      <description>Search algorithms help find the correct sequence of actions in a search space, to reach a goal state. The sequence of actions might be:
 Sequence in which cities are to be visited to travel from a source to a destination under a given cost function (shortest path, cheapest fare etc.) Sequence in which an agent should play moves in a game (chess, tic tac toe, pacman etc.) to win a board game Sequence in which a robot arm should solder components on a PCB under a given cost function (e.</description>
    </item>
    
    <item>
      <title>About</title>
      <link>http://teleported.in/about/about/</link>
      <pubDate>Sun, 30 Apr 2017 01:49:36 +0530</pubDate>
      
      <guid>http://teleported.in/about/about/</guid>
      <description>I mostly blog about intuition and concepts in Deep Learning and Computer Vision.
I am on a journey to explore these topics. I find them fascinating. I take notes as I read, listen, learn. These blog posts are extensions of my notes. These are mostly for me to read later. You may find them helpful as well.
Currently I am attending the fast.ai course (2017 cohort) via their international fellowship program.</description>
    </item>
    
  </channel>
</rss>